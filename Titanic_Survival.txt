



---

# Bivariate Graphical EDA 


Looking at our variables again:

```{r}
names(train)
```

There are $(n * (n-1)) / 2 = (11 * 10)/2 = 55$ possible bivariate combinations (regardless of order) of our 11 variables. We can compute bivariate and higher-order combinations with the `combn()` function:

```{r}
# combinations
head(t(data.frame(combn(11, 2))))
tail(t(data.frame(combn(11, 2))))
```

One way to plot all of these at once is using a scatterplot matrix. The `plot()` function will do this in R, when passed a **data frame**. Since it is hard to visualize 55 combinations, let's narrow down to a few choice attributes:


```{r fig.height=6, fig.width=8.5}
# Scatterplot matrix
chosen <- c("SurvivedNum", "Pclass", "Sex","Age","Fare","Embarked")
plot(train[,colnames(train) %in% chosen])
```


Notice how numerical attributes (`Age` and `Fare`) combine well into a scatterplot, yet other attributes are not plotted exactly as we might want. Since the problem space will only increase with higher-dimensional combinations, we select only a few choice pairs to consider. One approach is to compare each of the other ten features with our `Survived` outcome.


## Interactions with Survival

* 1 Survived & Pclass

A mosaic plot shows neatly this interaction:

```{r fig.height=4, fig.width=7, echo=FALSE}
# 1. Survived and Pclass 
suppressMessages(require(ggplot2))
suppressMessages(require(ggmosaic))
Survival <- ifelse(train$SurvivedNum==1,"yes","no") # for ggplot
PclassFac <- factor(train$Pclass)
ggplot(data=train) +
   geom_mosaic(aes(x=product(Survival, PclassFac),fill=Survival)) +
   labs(x='Passenger Class', y='', title='Tianic Survival by Passenger Class')
```


It would seem that folks in first class and second class had it better than those in third class. What the mosaic plot shows is also the comparative size of the populations of these three classes (in our training sample of course).


* 2 Survived & Sex

```{r fig.height=4, fig.width=6, echo=FALSE}
# 2 Survived & Sex
ggplot(data=train) +
   geom_mosaic(aes(x=product(Survival, GenderFac),fill=Survival)) +
   labs(x='Sex', y='', title='Tianic Survival by Gender')
```

Females were much more likely to survive, and the majority of the passengers was male.

* 3 Survived & Age

```{r fig.height=5, fig.width=8.5, echo=FALSE}
# 3 Survived & Age
plot(train$SurvivedNum~train$Age, pch=19, col=rgb(0,0,.6,.2),
    main="Titanic Survival by Age",ylab="Probability of Survival", xlab="Age")
linmod=lm(SurvivedNum~Age,data=train)
abline(linmod, col="green", lwd=2, lty=2)
g=glm(SurvivedNum~Age,family='binomial',data=train)
curve(predict(g,data.frame(Age=x),type="resp"),col="red",lty=2,lwd=2,add=TRUE) 
legend(60,0.7,c("linear fit","logistic fit"), col=c("green","red"), lty=c(1,2))
```

As expected, the probability of survival declines with age, as shown by the linear fit, which is quite similar to the logistic fit (a sinusoidal curve) as survival is not rare and the distribution of ages is roughly normal (as we noted in the univariate EDA), so we observe mid-range probabilities.


* 4 Survived & SibSp


```{r fig.height=5, fig.width=8.5, echo=FALSE}
# 4 Survived & SibSp
ggplot(data=train) +
   geom_mosaic(aes(x=product(Survival, SibSp),fill=Survival)) +
   labs(x='Number of Siblings/Spouses', y='', 
   title='Tianic Survival by Number of Siblings or Spouses')
```


Having one sibling or spouse is most indicative of survival, followed by two, then none, then four and up. The probability of survival is very low for higher numbers but our confidence that this is the case should decrease because there is gradually less evidence for this effect, given the smaller sample sizes as shown in the mosaic plot.


* 5 Survived & Parch

```{r fig.height=5, fig.width=8.5, echo=FALSE}
# 5 Survived & Parch
ggplot(data=train) +
   geom_mosaic(aes(x=product(Survival, Parch),fill=Survival)) +
   labs(x='Number of Parents/Children', y='', 
   title='Tianic Survival by Number of Parents or Children')
```

Similar results to those observed in the previous plot are seen, except for the higher probability of survival for someone with 3 (presumably) children, yet again, since the sample sizes are small, we should not take this finding too seriously.

When **feature engineering** we will take into account these findings to select the best method to create our indicator variables.

   
* 6 Survived & Fare 


```{r fig.height=5, fig.width=8.5, echo=FALSE}
# 6 Survived & Fare
plot(train$SurvivedNum~train$Fare, pch=19, col=rgb(0,0,.6,.2),
    main="Titanic Survival by Fare",ylab="Probability of Survival", xlab="Fare (Pounds Sterling)")
linmod=lm(SurvivedNum~Fare,data=train)
abline(linmod, col="green", lwd=1, lty=2)
g=glm(SurvivedNum~Fare,family='binomial',data=train)
curve(predict(g,data.frame(Fare=x),type="resp"),col="red",lty=2,lwd=2,add=TRUE) 
legend(200,0.7,c("linear fit","logistic fit"), col=c("green","red"), lty=c(1,2))
```

Unlike the plot of Survival by Age, we observe extreme probabilities given the skewed distribution of Fare, which shows how survival is increasingly more probable the higher the fare.

We can explore creating a log of Fare which could be used in modeling, as some models (i.e. linear models) would benefit from this logged variable as opposed to the original Fare attribute.

```{r fig.height=5, fig.width=8.5, echo=FALSE}
# 6 Survived & Log(Fare)
train$FareLog <- log(train$Fare+1) # adding a pound to avoid Inf log values for 0 fares
# plot
plot(train$SurvivedNum~train$FareLog, pch=19, col=rgb(0,0,.6,.2),
    main="Titanic Survival by Log of Fare",
    ylab="Probability of Survival", xlab="Fare in Log(Pounds Sterling)")
linmod=lm(SurvivedNum~FareLog,data=train)
abline(linmod, col="green", lwd=1, lty=2)
g=glm(SurvivedNum~FareLog,family='binomial',data=train)
curve(predict(g,data.frame(FareLog=x),type="resp"),col="red",lty=2,lwd=2,add=TRUE) 
legend(1,0.7,c("linear fit","logistic fit"), col=c("green","red"), lty=c(1,2))
```


The `FareLog` variable will indeed be useful for linear modeling.


* 7 Survived & Cabin

Since our data has so many missing values for cabin, our confidence in the results of this plot should be decreased.

```{r fig.height=5, fig.width=8.5, echo=FALSE}
# 7 Survived & Cabin
train2 <- train[!is.na(train$Cabin),] # copy of train w/o NAs
Survival2 <- ifelse(train2$SurvivedNum==1,"yes","no") 
ggplot(data=train2) +
   geom_mosaic(aes(x=product(Survival2, Cabin),fill=Survival2)) +
   labs(x='Cabin', y='', title='Tianic Survival by Cabin')
```


It would appear that perhaps cabin is not as associated with survivability as we had hoped for, given that A cabins are on the deck and F cabins near the keel where the ship hit the iceberg.


* 8 Survived & Embarked 

```{r fig.height=5, fig.width=8.5, echo=FALSE}
# 8 Survived & Embarked 
PortEmbarked <- ifelse(train$Embarked=="C","Cherbourg", 
                ifelse(train$Embarked=="Q","Queenstown", "Southampton"))
dat <- data.frame(Survival, PortEmbarked)

ggplot(data=dat) +
   geom_mosaic(aes(x=product(Survival, PortEmbarked),fill=Survival)) +
   labs(x='Port of Embarkation', y='', 
   title='Tianic Survival by Port of Embarkation')
```

There seems to be some evidence that having embarked in Southhampton is an indicator of higher probability of survival. 

We can explore port of embarkation in a more nuanced manner by considering the fares paid at each port, and whether survivability appears to me more associated with the fare or the port embarked. We use the log of fares since it would be hard to observe any differences in the boxplots given the highly skewed distribution of fare.


```{r fig.height=5, fig.width=9, echo=FALSE}
# 8 Survived & Embarked & Fare
dat$FareLog <- train$FareLog
dat <- dat[!is.na(dat$PortEmbarked),]
ggplot(data=dat) +
   geom_boxplot(aes(x=PortEmbarked,y=FareLog, fill=Survival)) +
   labs(x='Port of Embarkation', y='Fare in Log(Pounds Sterling)', 
   title='Titanic Survival by Port of Embarkation and Fare')
```


Several curiosities pop out in this plot. First, Southhampton's higher survivability is not entirely associated with fare, since Cherbourg seems to have a higher survivability when considering fare. Second, Queenstown's seems to go against common sense in that higher fares aren't necessarily associated with higher survivability. Lastly, the difference in survivability according to fare varies from port to port, for example, we see a more pronounced difference in Cherbourg, and almost no difference in Queenstown.



* 9 Survival and Title 

```{r fig.height=5, fig.width=9, echo=FALSE}
# 9 Survival and Title
ggplot(data=train) +
   geom_mosaic(aes(x=product(Survival, Title),fill=Survival)) + 
   labs(x='Title', y='',
   title='Tianic Survival by Title') + 
   theme(axis.text.x = element_text(angle = 90))
```

Title can be seen as a proxy for gender and as we've seen, females survived a lot better than males. It is worth keeping this attribute as it shows some granularity in what kinds of folks survived better within gender groups, i.e. those with rare titles.


* 10 Survived and NameLength

```{r fig.height=4.5, fig.width=8, echo=FALSE}
# 10 Survived and NameLength
plot(train$SurvivedNum~train$NameLength, pch=19, col=rgb(0,0,.6,.2),
    main="Titanic Survival by Name Length",
    ylab="Probability of Survival", xlab="Name Length (chars)")
linmod=lm(SurvivedNum~NameLength,data=train)
abline(linmod, col="green", lwd=1, lty=2)
g=glm(SurvivedNum~NameLength,family='binomial',data=train)
curve(predict(g,data.frame(NameLength=x),type="resp"),col="red",lty=2,lwd=2,add=TRUE) 
legend(60,0.5,c("linear fit","logistic fit"), col=c("green","red"), lty=c(1,2))
```


Longer names do appear to have some association with higher probabilities of survival so we are also keeping this feature. It might just be capturing the association of longer names and wealth, but since in machine learning we do not care about multicollinearity issues, we will test whether to keep this attribute or not during our feature selection modeling phase.

---




# Multivariate Graphical EDA

The higher-dimensional problem space of combinations with 11 variables is as follows:

```{r}
# Combinations of 3 or more variables quickly explode
vars <- 1:11
for (i in 2:9) {
	num <- length(combn(vars,i))/i
	print(paste("There are ", num, "combinations of 11 variables taken", i, "at a time."))
}
```

The number of combinations is complementary (adding up to 11): 6 = 5, 7 = 4, etc., so when considering combinations of 9 variables, we are just considering the complement of 2 variables.

Of the 165 trivariate combinations, 45 alone are possibilities that interact with our outcome `Survival` -- clearly too many to consider, so our approach will by means of necessity be minimal and select, but I wanted to make the size of the entire enterprise known.




# Conclusion

